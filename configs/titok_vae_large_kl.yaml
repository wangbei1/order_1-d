experiment:
    # tokenizer_checkpoint: "ckpt/tokenizer_titok_l32.bin"
    # generator_checkpoint: "ckpt/generator_titok_l32.bin"

model:
    vq_model:
        codebook_size: 4096
        token_size: 12
        use_l2_norm: True
        commitment_cost: 0.25
        # vit arch
        vit_enc_model_size: "large"
        vit_dec_model_size: "large"
        vit_enc_patch_size: 16
        vit_dec_patch_size: 16
        num_latent_tokens: 32
    lossconfig:
      target: modeling.contperceptual.LPIPSWithDiscriminator
      params:
        disc_start: 50001
        kl_weight: 0.000001
        disc_weight: 0.5
    optimizer:
        learning_rate: 1.0e-04
        scheduler_type: "linear-warmup" #"linear-warmup_cosine-decay"
        warmup_epochs: 0.1

    
    

dataset:
    preprocessing:
        crop_size: 256