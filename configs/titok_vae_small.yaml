# experiment:
#     tokenizer_checkpoint: "ckpt/tokenizer_titok_l32.bin"
#     generator_checkpoint: "ckpt/generator_titok_l32.bin"

model:
    vq_model:
        codebook_size: 4096
        token_size: 12
        use_l2_norm: True
        commitment_cost: 0.25
        # vit arch
        vit_enc_model_size: "small"
        vit_dec_model_size: "small"
        vit_enc_patch_size: 16
        vit_dec_patch_size: 16
        num_latent_tokens: 32
    lossconfig:
      target: modeling.vqperceptual.VQLPIPSWithDiscriminator
      params:
        disc_start: 0
        codebook_weight: 1.0
        pixelloss_weight: 1.0
        disc_factor: 1.0
        disc_weight: 1.0
        perceptual_weight: 1.0
        disc_conditional: false
        disc_loss: hinge
        disc_weight_max: 0.75
    optimizer:
        learning_rate: 1.0e-04
        scheduler_type: "linear-warmup" #"linear-warmup_cosine-decay"
        warmup_epochs: 0.1

    
    

dataset:
    preprocessing:
        crop_size: 256